{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook is used to serialize collected data onto disc.\n",
    "#The serialized dataframe contains only essential columns.\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.insert(0, '../../syne-tune/benchmarking/examples/benchmark_hypertune')\n",
    "from benchmark_definitions import benchmark_definitions\n",
    "import ipynb_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment\n",
    "#df = ipynb_utils.load_experiment_synetune('benchtest-1')\n",
    "df = ipynb_utils.load_experiment_synetune('baselines-4')\n",
    "\n",
    "algorithm_to_remove = 'DyHPO-lim'\n",
    "\n",
    "# Remove the algorithm\n",
    "df = df.drop(algorithm_to_remove, level='algorithm')\n",
    "print(df.info())\n",
    "\n",
    "# Remove extra benchmarks (too low runtime)\n",
    "benchmarks_to_remove= ['lcbench-jannis', 'lcbench-volkert']\n",
    "df = df.drop(benchmarks_to_remove, level='benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index={'HYPERTUNE-INDEP': 'HYPERTUNE', 'MOBSTER-JOINT': 'MOBSTER', 'SYNCHB': 'Hyperband'}, inplace=True)\n",
    "print(f\"Benchmarks: {df.index.get_level_values('benchmark').unique()}\")\n",
    "print(f\"Algorithms: {df.index.get_level_values('algorithm').unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all missing columns and values into the dataframe\n",
    "new_dfs = []\n",
    "\n",
    "benchmark_names = df.index.get_level_values('benchmark').unique().tolist()\n",
    "budgets = {'n' : 4000, 'l' : 1000, 'f' : 2000}\n",
    "\n",
    "for benchmark_name in benchmark_names:\n",
    "    benchmark = benchmark_definitions[benchmark_name]\n",
    "    budget = budgets[benchmark_name[0]]\n",
    "    # Load correct subset of the dataframe\n",
    "    print(f\"Calculating {benchmark_name}\")\n",
    "    bench_df = df.loc[(benchmark_name, slice(None), slice(None), slice(None))]\n",
    "    bench_df.drop(columns=['metric_mode'], inplace=True)\n",
    "    bench_df['max_num_evaluations'] = budget\n",
    "    bench_df['optimization_metric'] = benchmark.metric\n",
    "    bench_df['mode'] = benchmark.mode\n",
    "    bench_df['metric'] = bench_df[benchmark.metric]\n",
    "    bench_df['benchmark'] = benchmark_name\n",
    "    bench_df.reset_index(inplace=True)\n",
    "    new_dfs.append(bench_df[['benchmark', 'algorithm', 'repeat', 'trial', 'max_num_evaluations', 'optimization_metric', 'mode', 'metric', 'elapsed_time', 'metric_elapsed_time', 'st_decision', 'trial_id']])\n",
    "\n",
    "new_df = pd.concat(new_dfs)\n",
    "new_df.reset_index(inplace=True)\n",
    "new_df.set_index(['benchmark', 'algorithm', 'repeat', 'trial'], inplace=True)\n",
    "new_df.drop(columns=['index'], inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of the data - cumulative metric and regret calculation\n",
    "def compute_cumulative_min(group):\n",
    "    mode = group['mode'].iloc[0]\n",
    "    if mode == 'min':\n",
    "        group['cumulative_min'] = group['metric'].cummin()\n",
    "        group['cumulative'] = group['metric'].cummin()\n",
    "    elif mode == 'max':\n",
    "        group['cumulative_min'] = group['metric'].cummax() * -1\n",
    "        group['cumulative'] = group['metric'].cummax()\n",
    "    return group.reset_index(['benchmark','algorithm','repeat'], drop=True)\n",
    "\n",
    "new_df = new_df.groupby(['benchmark', 'algorithm', 'repeat']).apply(compute_cumulative_min)\n",
    "\n",
    "# Calculate regret for the Wilcoxon test - normalized cumulative metric to 0-1 range\n",
    "new_df['regret'] = new_df.groupby(['benchmark'])['cumulative_min'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the dataframe\n",
    "new_df.to_feather('../../results/tabular-1.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".st_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
