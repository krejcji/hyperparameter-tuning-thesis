
\chapwithtoc{Conclusion}

% Where in the research map is my thesis - mention some work that has been done
%       - on HPO in general
%       - on comparisons
% To what extent did we respond to the gap in research

% We did perform quite a lot of new experiments, more than would be possible on just a single personal computer.

%  One such example is DyHPO, which dynamically selects the most promising configuration to evaluate, potentially after each training epoch.

Numerous hyperparameter optimization algorithms are available for deployment, many of which focus on efficiency through multi-fidelity evaluations. However, selecting the right method can be challenging, especially when most of the performance comparisons available are from the original research papers that introduce the algorithms. Moreover, the comparisons are often not very detailed, and authors choose which tasks and methods to include. In this thesis, we conducted experiments to provide independent and varied comparisons of the state-of-the-art algorithms to help practitioners make better decisions.

We found that by using an advanced algorithm, it is often possible to obtain a solution comparable to a solution found by random search, but using just a fraction of the budget. On tabular experiments, DyHPO achieved an average speedup factor of $6.92$ compared to random search. DyHPO proved to be one of the best algorithms in the comparison, achieving the best average rank on both tabular and real-world experiments, closely followed by HyperTune. One thing to note is that DyHPO can have a large overhead depending on the implementation. The results of the real-world experiments were more varied, which is why including diverse tasks and architectures in comparisons is needed. We did not find any major discrepancies between tabular and real-world experiments, suggesting that tabular benchmarks still provide a good general estimate of performance.

However, some limitations are worth noting. Most importantly, the data obtained from the real-world experiments were noisy, and 10 repetitions were insufficient to uncover more statistically supported findings. Moreover, the problems solved in the real-world experiments were still orders of magnitude simpler than some problems currently solved by machine learning, even though we included more recent architectures and often larger search spaces than tabular benchmarks. Future work should therefore focus on extending the diversity of tasks, e.g.\ by including NLP or reinforcement learning tasks; and performing more repetitions for more reliable results. Moreover, creating a new set of tabular benchmarks with more recent problems would also be valuable, as tabular benchmarks are useful not only for comparisons but more importantly for the development of new algorithms.
