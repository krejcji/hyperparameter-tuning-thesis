
\chapwithtoc{Introduction}
% Zadani
% Mnoho metod strojového učení je citlivých na nastavení hyper-parametrů. V posledních letech se rychle rozvíjí metody pro ladění tohoto nastavení. Většina existujících metod je ale velmi výpočetně náročná, což komplikuje jejich reálné nasazení. Zároveň chybí podrobnější porovnání výkonnosti jednotlivých metod. Cílem práce je porovnat existující metody pro ladění hyper-parametrů a navrhnout nové metody s ohledem nejen na kvalitu nalezených nastavení, ale také na dobu běhu.

% Student nastuduje dostupné metody a knihovny pro ladění hyper-parametrů metod strojového učení. Vybrané metody mezi sebou porovná a na základě získaných informací implementuje metody nové, které budou uvažovat jak rychlost ladění tak kvalitu nalezených řešení.

 %Establish the importance of the field
 %   1 Establish the importance of this research topic
 %         Hyperparameter optimization is important because
%
 %   2 Provide general background information
 %       The two most common approaches to hyperparameter tuning are by hand, or using a random search.
 %   3 Provide more detailed background information
 %   4 Describe the general problem area or the current research focus of the field
 %Previous or current research
 %   5 Provide a transition between the general problem area and the literature review
 %   6 Provide a brief overview of key research projects in this area
 %Locate a gap in research
 %   7 Describe a gap in the research
 %Describe the present paper
 %   8 Describe the paper itself
 %   9 Give details about the methodology reported in the paper
 %   10 Announce the findings

% Introduction should answer the following questions, ideally in this order

% What is the nature of the problem the thesis is addressing?
Hyperparameter optimization is an important step in the machine learning workflow that can have a striking effect on model generalization and performance. Historically, the most prevalent approaches to hyperparameter optimization were tuning the hyperparameters manually, using a random search, or a grid search. A breakthrough in the field was the adaptation of Bayesian optimization~\cite{mockus1974bayesian,snoek2012practical}, which uses probabilistic models to predict the performance of hyperparameter configurations to make an informed decision about which configuration to evaluate next. Nevertheless, as the complexity of machine learning models has increased rapidly in recent years, not even Bayesian optimization alone might be feasible for many tasks. The next step seems to be the multi-fidelity hyperparameter tuning, which makes use of cheap but unreliable evaluations, often combined with Bayesian optimization.

% What is the common approach for solving that problem now?
% How this thesis approaches the problem?
% What are the results? Did something improve?
We conduct a literature survey on state-of-the-art hyperparameter optimization methods. Additionally, we examine the underlying principles of these methods, explaining the most important ideas and their practical implications. Our research found that limited performance comparisons of hyperparameter optimization algorithms were available. Therefore, instead of developing a new algorithm, we carried out experiments to compare the most promising algorithms across various tasks. These tasks include tabular benchmarks as well as real-world deep-learning problems. In addition to the usual image datasets, we chose two datasets from the healthcare domain, too. The results of the experiments confirm that multi-fidelity techniques are superior to random search, but no single algorithm consistently outperforms others across all tasks.

% that will do well on any problem?
% add more information about the multi-fidelity
% add more about the experiments, include healthcare domain

% What can the reader expect in the individual chapters of the thesis?
The thesis is divided into four chapters. In Chapter~1 we introduce key concepts and definitions that will be used throughout the thesis. These include a brief introduction to machine learning, a formal definition of the hyperparameter optimization problem, and hyperparameter optimization techniques including Bayesian optimization. In Chapter~2 we explore multi-fidelity optimization techniques in depth. Chapter~3 discusses the methodology, which includes the design of the experiments and how the collected data from the experiments are processed. In Chapter~4 we present the results and interpret them in the discussion.


% Expected length of the introduction is between 1--4 pages. Longer introductions may require sub-sectioning with appropriate headings --- use \texttt{\textbackslash{}section*} to avoid numbering (with section names like `Motivation' and `Related work'), but try to avoid lengthy discussion of anything specific. Any ``real science'' (definitions, theorems, methods, data) should go into other chapters.
% \todo{You may notice that this paragraph briefly shows different ``types'' of `quotes' in TeX, and the usage difference between a hyphen (-), en-dash (--) and em-dash (---).}

% It is very advisable to skim through a book about scientific English writing before starting the thesis. I can recommend `\citetitle{glasman2010science}' by \citet{glasman2010science}.