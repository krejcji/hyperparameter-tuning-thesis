% Universal approximation theorem
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@inproceedings{eggensperger2013towards,
  title={Towards an empirical foundation for assessing bayesian optimization of hyperparameters},
  author={Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger and Leyton-Brown, Kevin and others},
  booktitle={NIPS workshop on Bayesian Optimization in Theory and Practice},
  volume={10},
  number={3},
  pages={1--5},
  year={2013}
}

% Surveys
@article{survey2020,
  title={On hyperparameter optimization of machine learning algorithms: Theory and practice},
  author={Yang, Li and Shami, Abdallah},
  journal={Neurocomputing},
  volume={415},
  pages={295--316},
  year={2020},
  publisher={Elsevier}
}

@article{survey2021,
  title={Hyperparameter optimization: Foundations, algorithms, best practices and open challenges. arXiv 2021},
  author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
  journal={arXiv preprint arXiv:2107.05847}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}


% Practical guides
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}


% Hyperparameter optimization frameworks
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@inproceedings{akiba2019optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@article{smac3,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

@inproceedings{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5},
  pages={507--523},
  year={2011},
  organization={Springer}
}

@article{hutter2010sequential,
  title={Sequential model-based optimization for general algorithm configuration (extended version)},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Technical Report TR-2010--10, University of British Columbia, Computer Science, Tech. Rep.},
  year={2010}
}


% Multi-fidelity optimization
@article{swersky2013multi,
  title={Multi-task bayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{swersky2014freeze,
  title={Freeze-thaw Bayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}

@inproceedings{domhan2015speeding,
  title={Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author={Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@InProceedings{jamieson16,
  title = 	 {Non-stochastic Best Arm Identification and Hyperparameter Optimization},
  author = 	 {Jamieson, Kevin and Talwalkar, Ameet},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {240--248},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/jamieson16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/jamieson16.html},
  abstract = 	 {Motivated by the task of hyperparameter optimization, we introduce the \em non-stochastic best-arm identification problem. We identify an attractive algorithm  for this setting that makes no assumptions on the convergence behavior of the arms’ losses, has no free-parameters to adjust, provably outperforms the uniform allocation baseline in favorable conditions, and performs comparably (up to \log factors) otherwise. Next, by leveraging the iterative nature of many  learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification. Our empirical results show that, by allocating more resources to promising hyperparameter settings, our approach achieves comparable test accuracies an order of magnitude faster than the uniform strategy. The robustness and simplicity of our approach makes it well-suited to ultimately replace the uniform strategy currently used in most machine learning software packages.}
}

@inproceedings{klein2017fast,
  title={Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author={Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle={Artificial intelligence and statistics},
  pages={528--536},
  year={2017},
  organization={PMLR}
}

@article{li2018hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={185},
  pages={1--52},
  year={2018}
}

@inproceedings{falkner2018bohb,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle={International conference on machine learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

@article{klein2020model,
  title={Model-based asynchronous hyperparameter and neural architecture search},
  author={Klein, Aaron and Tiao, Louis C and Lienart, Thibaut and Archambeau, Cedric and Seeger, Matthias},
  journal={arXiv preprint arXiv:2003.10865},
  year={2020}
}

@article{li2020system,
  title={A system for massively parallel hyperparameter tuning},
  author={Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-Tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={230--246},
  year={2020}
}

@article{awad2021dehb,
  title={Dehb: Evolutionary hyperband for scalable, robust and efficient hyperparameter optimization},
  author={Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
  journal={arXiv preprint arXiv:2105.09821},
  year={2021}
}

@inproceedings{zhu2020accelerating,
  title={Accelerating hyperparameter optimization of deep neural network via progressive multi-fidelity evaluation},
  author={Zhu, Guanghui and Zhu, Ruancheng},
  booktitle={Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11--14, 2020, Proceedings, Part I 24},
  pages={752--763},
  year={2020},
  organization={Springer}
}

@article{zimmer2021auto,
  title={Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl},
  author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={9},
  pages={3079--3090},
  year={2021},
  publisher={IEEE}
}

@article{li2022hyper,
  title={Hyper-tune: Towards efficient hyper-parameter tuning at scale},
  author={Li, Yang and Shen, Yu and Jiang, Huaijun and Zhang, Wentao and Li, Jixiang and Liu, Ji and Zhang, Ce and Cui, Bin},
  journal={arXiv preprint arXiv:2201.06834},
  year={2022}
}

@article{wistuba2022supervising,
  title={Supervising the multi-fidelity race of hyperparameter configurations},
  author={Wistuba, Martin and Kadra, Arlind and Grabocka, Josif},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13470--13484},
  year={2022}
}

@article{yu2024two,
  title={Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset},
  author={Yu, Sungduk and Ma, Po-Lun and Singh, Balwinder and Silva, Sam and Pritchard, Mike},
  journal={Artificial Intelligence for the Earth Systems},
  volume={3},
  number={1},
  pages={e230013},
  year={2024},
  publisher={American Meteorological Society}
}

@inproceedings{mirzasoleiman2020coresets,
  title={Coresets for data-efficient training of machine learning models},
  author={Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure},
  booktitle={International Conference on Machine Learning},
  pages={6950--6960},
  year={2020},
  organization={PMLR}
}


% Transfer learning
@inproceedings{bardenet2013collaborative,
  title={Collaborative hyperparameter tuning},
  author={Bardenet, R{\'e}mi and Brendel, M{\'a}ty{\'a}s and K{\'e}gl, Bal{\'a}zs and Sebag, Michele},
  booktitle={International conference on machine learning},
  pages={199--207},
  year={2013},
  organization={PMLR}
}

@inproceedings{yogatama2014efficient,
  title={Efficient transfer learning method for automatic hyperparameter tuning},
  author={Yogatama, Dani and Mann, Gideon},
  booktitle={Artificial intelligence and statistics},
  pages={1077--1085},
  year={2014},
  organization={PMLR}
}

@article{perrone2018scalable,
  title={Scalable hyperparameter transfer learning},
  author={Perrone, Valerio and Jenatton, Rodolphe and Seeger, Matthias W and Archambeau, C{\'e}dric},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wang2021pre,
  title={Pre-trained Gaussian processes for Bayesian optimization},
  author={Wang, Zi and Dahl, George E and Swersky, Kevin and Lee, Chansoo and Nado, Zachary and Gilmer, Justin and Snoek, Jasper and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:2109.08215},
  year={2021}
}

% Bayesian optimization
@inproceedings{mockus1974bayesian,
  title={On Bayesian methods for seeking the extremum},
  author={Mockus, Jonas},
  booktitle={Proceedings of the IFIP Technical Conference},
  pages={400--404},
  year={1974}
}

 % Expected improvement
@article{jones1998efficient,
  title={Efficient global optimization of expensive black-box functions},
  author={Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal={Journal of Global optimization},
  volume={13},
  pages={455--492},
  year={1998},
  publisher={Springer}
}

 % Probability of improvement
@article{kushner1964new,
  title={A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
  author={Kushner, Harold J},
  year={1964}
}

@book{rasmussen2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{wang2023intuitive,
  title={An intuitive tutorial to Gaussian processes regression},
  author={Wang, Jie},
  journal={Computing in Science \& Engineering},
  year={2023},
  publisher={IEEE}
}

@article{brochu2010tutorial,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{watanabe2023tree,
  title={Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance},
  author={Watanabe, Shuhei},
  journal={arXiv preprint arXiv:2304.11127},
  year={2023}
}

% Benchmarks

NAS-Bench-201
@inproceedings{dong2020nasbench201,
  title     = {NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search},
  author    = {Dong, Xuanyi and Yang, Yi},
  booktitle = {International Conference on Learning Representations (ICLR)},
  url       = {https://openreview.net/forum?id=HJxyZkBKDr},
  year      = {2020}
}

LCBench
@article {ZimLin2021a,
  author = {Lucas Zimmer and Marius Lindauer and Frank Hutter},
  title = {Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2021},
  volume = {43},
  number = {9},
  pages = {3079 - 3090}
}

% FCNet
@article{klein2019tabular,
  title={Tabular benchmarks for joint architecture and hyperparameter optimization},
  author={Klein, Aaron and Hutter, Frank},
  journal={arXiv preprint arXiv:1905.04970},
  year={2019}
}

% Stats
@article{demvsar2006statistical,
  title={Statistical comparisons of classifiers over multiple data sets},
  author={Dem{\v{s}}ar, Janez},
  journal={The Journal of Machine learning research},
  volume={7},
  pages={1--30},
  year={2006},
  publisher={JMLR. org}
}

% Syne Tune
@inproceedings{
    salinas2022syne,
    title = {{Syne Tune}: A Library for Large Scale Hyperparameter Tuning and Reproducible Research},
    author = {David Salinas and Matthias Seeger and Aaron Klein and Valerio Perrone and Martin Wistuba and Cedric Archambeau},
    booktitle = {International Conference on Automated Machine Learning, AutoML 2022},
    year = {2022},
    url = {https://proceedings.mlr.press/v188/salinas22a.html}
}

@article{strodthoff2020deep,
  title={Deep learning for ECG analysis: Benchmarks and insights from PTB-XL},
  author={Strodthoff, Nils and Wagner, Patrick and Schaeffter, Tobias and Samek, Wojciech},
  journal={IEEE journal of biomedical and health informatics},
  volume={25},
  number={5},
  pages={1519--1528},
  year={2020},
  publisher={IEEE}
}


% Neural Architectures
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

% xResnet original paper
@article{he2018xresnet,
  author       = {Tong He and
                  Zhi Zhang and
                  Hang Zhang and
                  Zhongyue Zhang and
                  Junyuan Xie and
                  Mu Li},
  title        = {Bag of Tricks for Image Classification with Convolutional Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1812.01187},
  year         = {2018},
  url          = {http://arxiv.org/abs/1812.01187},
  eprinttype    = {arXiv},
  eprint       = {1812.01187},
  timestamp    = {Tue, 21 Mar 2023 20:02:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1812-01187.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

% xResnet1d implementation
@misc{xresnet2023github,
  author       = {Ing-Yang Jang},
  title        = {xResnet1d},
  howpublished = {\url{https://github.com/hawkiyc/xResnet1d}},
  note         = {[Accessed: 2024-07-11]},
}

% TorchXRayVision
@inproceedings{Cohen2022xrv,
title = {{TorchXRayVision: A library of chest X-ray datasets and models}},
author = {Cohen, Joseph Paul and Viviano, Joseph D. and Bertin, Paul and Morrison, Paul and Torabian, Parsa and Guarrera, Matteo and Lungren, Matthew P and Chaudhari, Akshay and Brooks, Rupert and Hashir, Mohammad and Bertrand, Hadrien},
booktitle = {Medical Imaging with Deep Learning},
url = {https://github.com/mlmed/torchxrayvision},
arxivId = {2111.00595},
year = {2022}
}

% Datasets
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Baolin and Ng, Andrew Y and others},
  booktitle={NIPS workshop on deep learning and unsupervised feature learning},
  volume={2011},
  number={2},
  pages={4},
  year={2011},
  organization={Granada}
}

@article{wagner2020ptb,
  title={PTB-XL, a large publicly available electrocardiography dataset},
  author={Wagner, Patrick and Strodthoff, Nils and Bousseljot, Ralf-Dieter and Kreiseler, Dieter and Lunze, Fatima I and Samek, Wojciech and Schaeffter, Tobias},
  journal={Scientific data},
  volume={7},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{wang2017chestx,
  title={Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
  author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2097--2106},
  year={2017}
}





